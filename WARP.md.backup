# WARP.md

This file provides guidance to WARP (warp.dev) when working with code in this repository.

## Purpose

Build a **neutral, reproducible analytics pipeline** to analyze a public Telegram channel and answer:

* Who the author **supports** and **opposes** over time.
* What **topics and issues** are discussed, including emerging ones.
* The **context** and **linguistic framing** of messages, including quotes and forwarded content.
* Which **external links/domains** are amplified.
* Produce outputs with traceable evidence to avoid over-attribution or bias.

The pipeline must prioritize **attribution accuracy**, **transparency**, and **config-driven repeatability**.

üìñ **Complete specification**: [`docs/spec.md`](docs/spec.md)

## Scope

### In-scope (Phase 1)

* Normalize Telegram data (JSON/CSV).
* Entity extraction and aliasing to canonical forms.
* Stance classification per entity, with context-aware attribution.
* Hybrid **topic classification**:

  * Stable ontology for time series reporting.
  * Dynamic discovery to capture emerging topics.
* Quote detection and proper speaker attribution.
* Sentiment, toxicity, and stylistic features.
* Link and domain extraction.
* Aggregation outputs and visual sidecars.
* GPU acceleration (MPS for Apple, CUDA for Nvidia).
* CLI + config file driven.

### Future extensions

* Sarcasm/irony detection.
* Multimedia analysis (OCR, audio).
* Cross-lingual stance with automatic routing.

## Guiding Principles

* **Neutrality by design**: Default to `unclear` when ambiguous
* **Transparency**: Evidence spans stored with every stance edge
* **Hybrid thinking**: Fixed ontology for stability, unsupervised discovery for novelty
* **Speaker-aware**: Never mix author words with quoted or forwarded voices
* **Iterative refinement**: Top entities and clusters logged for alias/topic updates

‚ö†Ô∏è **Attribution accuracy is critical** - avoid over-interpretation of quoted material.

## Quickstart (TL;DR)

```bash
# Clone and setup
git clone <repository-url> && cd raigem0n
pyenv install 3.11.9 && pyenv local 3.11.9
python -m venv .venv && source .venv/bin/activate
pip install -U pip && pip install -r requirements.txt
python -m spacy download en_core_web_sm

# Configure for CPU (default per Exo rules)
export CLANG=1; unset CUDA; unset GPU

# Run analysis
python telegram_analyzer.py \
  --config config/config.yaml \
  --input data/sample_channel.json \
  --out out/run-$(date +%Y%m%d-%H%M)

# View outputs
ls out/run-*/
```

## Prerequisites and System Requirements

**Operating Systems:**
- Ubuntu 22.04 LTS, macOS 13+, Windows via WSL2

**Core Requirements:**
- Python 3.11.x (via pyenv preferred)
- Git, pyenv, make, curl/wget
- ‚â•8GB RAM, ‚â•10GB disk space for models
- Network access to Hugging Face Hub

**Optional GPU Acceleration:**
- NVIDIA: CUDA 12.x toolkit
- Apple Silicon: MPS (built-in)

**Installation Links:**
- [pyenv](https://github.com/pyenv/pyenv#installation)
- [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit)

## Environment Setup (pyenv-first)

```bash
# Install Python and create isolated environment
pyenv install 3.11.9
pyenv local 3.11.9
python -m venv .venv
source .venv/bin/activate

# Upgrade pip and install pinning tools
pip install -U pip pip-tools pip-audit

# Install dependencies (after requirements.txt exists)
pip install -r requirements.txt

# Security audit
pip-audit

# Download required models
python -m spacy download en_core_web_sm
```

**Package Pinning Strategy:**
- Use `requirements.in` ‚Üí `pip-compile` ‚Üí `requirements.txt`
- Run `pip-audit` before deployment
- Pin major ML model versions in config

**Optional Development Tools:**
```bash
pip install pre-commit ruff black mypy bandit
pre-commit install  # if .pre-commit-config.yaml exists
```

## Hardware Acceleration Profiles

### CPU/CLANG (Default)
```bash
export CLANG=1
unset CUDA
unset GPU
# In config.yaml: processing.prefer_gpu: false
```

### NVIDIA CUDA
```bash
# Verify CUDA installation
python -c "import torch; print(torch.cuda.is_available())"

export CUDA_VISIBLE_DEVICES=0
# In config.yaml: processing.prefer_gpu: true
```

### Apple Silicon MPS
```bash
export PYTORCH_ENABLE_MPS_FALLBACK=1
# In config.yaml: processing.prefer_gpu: true
```

**Note:** Pipeline automatically falls back to CPU if GPU unavailable at runtime.

## Data Model and Normalization

### Input Schema

Columns:
* `msg_id` (unique)
* `chat_id` (sender or channel ID)
* `date` (ISO8601 string)
* `text` (normalized message body)
* `media_type`, `media_url` (optional)
* `forwarded_from` (optional, for attribution)

**Supported Formats:** JSON, JSONL, CSV

**Sample Input Structure:**
```json
{
  "msg_id": "12345",
  "chat_id": "@channel",
  "date": "2024-01-15T14:30:00Z",
  "text": "The author discusses policy X...",
  "forwarded_from": null
}
```

### Output Schema (per message)

| Column            | Type   | Description                                       |
| ----------------- | ------ | ------------------------------------------------- |
| `entities`        | JSON   | Extracted entities with canonical mapping         |
| `stance`          | JSON   | `{speaker, target, label, score, evidence_spans}` |
| `topics`          | JSON   | Scored list of topics                             |
| `top_topic_label` | string | Highest scoring topic                             |
| `sentiment_label` | string | Positive/Negative/Neutral                         |
| `sentiment_score` | float  | Confidence                                        |
| `toxicity_score`  | float  | Confidence                                        |
| `links`           | JSON   | URLs found                                        |
| `domains`         | JSON   | Parsed domains                                    |
| `language`        | string | Detected language                                 |
| `style_features`  | JSON   | Exclamation count, all-caps ratio, hedges, etc.   |

### Processing Rules

* Support JSON, JSONL, or CSV formats
* Coerce Telegram's `list-of-spans` format into a single string
* Limit text length (default 8k characters)
* Normalize entities to `{PERSON, ORG, LOC, MISC}`
* Deduplicate mentions per message

## Configuration

### Main Config (`config/config.yaml`)
```yaml
io:
  input_path: data/channel.json
  format: json
  text_col: message
  id_col: message_id
  date_col: date
  out_path: out/posts_enriched.parquet

models:
  ner: dslim/bert-base-NER
  sentiment: cardiffnlp/twitter-roberta-base-sentiment-latest
  toxicity: unitary/toxic-bert
  stance: facebook/bart-large-mnli
  topic: facebook/bart-large-mnli

processing:
  batch_size: 32
  prefer_gpu: true
  quote_aware: true
  skip_langdetect: true
  max_entities_per_msg: 3
  stance_threshold: 0.6

resources:
  aliases_path: config/aliases.json
  topics_path: config/topics.json

# Extended configuration with all options:
topic:
  ontology_path: config/topics.json
  discovery:
    enabled: true
    method: kmeans           # hdbscan optional via extra
    max_topics: 30
    min_cluster_size: 20
    labeler: tfidf_keywords  # or keybert if enabled

processing:
  stance:
    threshold: 0.6
    dep_rules_weight: 0.6
    mnli_weight: 0.4
    negative_words: [not, never, no]
    support_verbs: [support, back, endorse, praise]
    oppose_verbs: [oppose, criticize, condemn, attack]
  
  quote:
    detect_forwarded: true
    detect_quoted_spans: true
    max_quote_length: 2048
    attribute_forwarded_to_source: true

io:
  parquet_engine: fastparquet   # pyarrow optional
  parquet_compression: zstd
```

### Entity Aliases (`config/aliases.json`)
```json
{
  "Donald Trump": {
    "aliases": ["Trump", "President Trump", "DJT"],
    "type": "PERSON",
    "id": "Q22686"
  },
  "Democratic Party": {
    "aliases": ["Democrats", "Dems"],
    "type": "ORG"
  }
}
```

### Topic Ontology (`config/topics.json`)
```json
[
  {
    "label": "immigration",
    "keywords": ["immigration", "border"]
  },
  {
    "label": "economy", 
    "keywords": ["jobs", "inflation"]
  }
]
```

## Running the Pipeline

### Basic Execution
```bash
python telegram_analyzer.py \
  --config config/config.yaml \
  --input data/channel.json \
  --out out/run-$(date +%Y%m%d-%H%M)
```

### Common Variants
```bash
# Skip language detection (monolingual data)
python telegram_analyzer.py --config config.yaml --skip-langdetect --input data.json

# Adjust batch size for available memory
python telegram_analyzer.py --config config.yaml --batch-size 16 --input data.json

# Force CPU execution
export CLANG=1; unset CUDA; unset GPU
python telegram_analyzer.py --config config.yaml --input data.json

# Enable verbose logging
python telegram_analyzer.py --config config.yaml --verbose --input data.json 2>&1 | tee run.log
```

### Model Caching
```bash
# Control Hugging Face cache location
export HF_HOME=/path/to/cache
export TRANSFORMERS_CACHE=/path/to/cache/transformers

# Pre-download models
python -c "from transformers import pipeline; pipeline('ner', 'dslim/bert-base-NER')"
```

## Aggregation Outputs

| File                         | Purpose                                    |
| ---------------------------- | ------------------------------------------ |
| `*_daily_summary.csv`        | Posts per day, avg sentiment, max toxicity |
| `*_entity_stance_counts.csv` | Total support/oppose per entity            |
| `*_entity_stance_daily.csv`  | Stance timeline per entity                 |
| `*_topic_share_daily.csv`    | Topic distribution over time               |
| `*_domain_counts.csv`        | Amplified domains                          |
| `*_top_toxic_messages.csv`   | Most toxic messages with context           |

## Outputs and Artifacts

**Directory Structure:**
```
out/run-20241215-1430/
‚îú‚îÄ‚îÄ posts_enriched.parquet          # Main output with all analysis
‚îú‚îÄ‚îÄ channel_daily_summary.csv       # Daily aggregated metrics
‚îú‚îÄ‚îÄ channel_entity_stance_counts.csv # Total stance per entity
‚îú‚îÄ‚îÄ channel_entity_stance_daily.csv  # Stance timeline
‚îú‚îÄ‚îÄ channel_topic_share_daily.csv    # Topic distribution over time
‚îú‚îÄ‚îÄ channel_domain_counts.csv        # Amplified domains
‚îú‚îÄ‚îÄ channel_top_toxic_messages.csv   # Most toxic content
‚îî‚îÄ‚îÄ config_snapshot.yaml             # Config used for this run
```

**Reading Outputs:**
```python
import pandas as pd
import pyarrow.parquet as pq

# Main enriched data
df = pd.read_parquet('out/run-*/posts_enriched.parquet')

# Stance evidence spans
stance_data = df['stance'].apply(pd.json_normalize)
print(stance_data[['speaker', 'target', 'label', 'score', 'evidence_spans']])

# Daily summaries
daily = pd.read_csv('out/run-*/channel_daily_summary.csv')
```

## Processing Stages

```mermaid
graph LR
    A[3.1 Load & Normalize] --> B[3.2 Language Detection]
    B --> C[3.3 NER]
    C --> D[3.4 Entity Aliasing]
    D --> E[3.5 Quote/Span Tagging]
    E --> F[3.6 Stance Classification]
    F --> G[3.7 Topic Analysis]
    G --> H[3.8 Sentiment/Toxicity]
    H --> I[3.9 Links/Domains]
    I --> J[3.10 Style Features]
    J --> K[3.11 Aggregations]
    K --> L[3.12 Output Writers]
```

### 3.1 Load & Normalize
* Support JSON, JSONL, or CSV formats
* Coerce Telegram's `list-of-spans` format into a single string
* Limit text length (default 8k characters)

### 3.2 Language Detection
* Default to `langdetect`
* Skip with `--skip-langdetect` if data is monolingual

### 3.3 NER
* Default: Hugging Face token-classification model (e.g., `dslim/bert-base-NER`)
* Optional: spaCy model (`en_core_web_sm`)
* Normalize to `{PERSON, ORG, LOC, MISC}`
* Deduplicate mentions per message

### 3.4 Entity Aliasing
* Load from `aliases.json`
* Map surface forms and nicknames ‚Üí canonical entities with optional IDs

### 3.5 Quote/Span Tagging
* Identify author vs. quoted/forwarded content
* Label spans as `author`, `quoted`, or `forwarded`
* Detect typographic quotes, block quotes, forwarded metadata

### 3.6 Stance Classification
* Hybrid approach: dependency rules + zero-shot MNLI
* Generate stance edges: `{speaker, target, label, score, evidence_spans}`
* Default: exclude non-author spans for stance attribution

### 3.7 Topic Analysis
* Ontology-based matching for stable categories
* Unsupervised discovery for emerging topics (embeddings + clustering)
* Map discovered clusters to ontology; label outliers as `NEW_TOPIC_X`

### 3.8 Sentiment & Toxicity
* RoBERTa-based classifiers for sentiment polarity and toxicity scoring
* Store both labels and confidence scores

### 3.9 Links & Domains
* Regex extract URLs
* Parse with `urlparse`
* Strip `www.` prefix
* Store both raw links and domain counts

### 3.10 Style Features
* Exclamation count (`!`)
* All-caps ratio
* Hedge words (e.g., "maybe", "perhaps")
* Superlatives ("best", "greatest")
* Persist in `style_features` JSON

### 3.11 Aggregations
* Daily summaries and timelines
* Entity stance totals and trends
* Topic distribution over time
* Domain amplification metrics

### 3.12 Output Writers
* Main output: Parquet file (default engine: fastparquet)
* CSV sidecars for specific views
* Config snapshot for reproducibility

**Key Design Rules:**
- **Neutrality by design**: default to `unclear` when ambiguous
- **Transparency**: evidence spans stored with every stance edge
- **Hybrid thinking**: fixed ontology for stability, unsupervised discovery for novelty
- **Speaker-aware**: never mix author words with quoted or forwarded voices
- **Iterative refinement**: top entities and clusters logged for alias/topic updates
- Cap entities per message (default: 3)
- Text clipping at 8k characters
- Batch processing with memory safeguards

## Quote Handling & Attribution

### Problem
Messages can contain:
- Author‚Äôs own words
- Quoted opponents/allies
- Forwarded messages
- Mixed attributions

Naive analysis falsely attributes quoted speech to the author.

### Solution
Treat messages as multi-speaker with span tagging:
- Quote detection:
  - Typographic quotes ("...", ‚Äú...‚Äù) and quoted block lines (prefix ">")
  - Forwarded metadata (forwarded_from)
  - Signature markers (‚Äî NAME)
- Span tagging:
  - Each sentence labeled `author`, `quoted(speaker=unknown|known)`, or `forwarded`
- Default stance rule:
  - Exclude non-author spans unless there‚Äôs explicit framing

### Configuration knobs
```yaml
processing:
  quote_aware: true
  quote:
    detect_forwarded: true
    detect_quoted_spans: true
    max_quote_length: 2048
    attribute_forwarded_to_source: true
```

### Evidence retention
Each stance edge stores evidence spans: `{start, end, text, speaker, span_type, source}`. This enables audits and prevents over-attribution.

## Contextual Stance Classification (Hybrid)

### Approach
1. Dependency-based rules (fast, interpretable):
   - Parse author spans with spaCy
   - Identify verbs/adjectives signaling stance: support, praise, condemn, oppose, criticize, corrupt, great
   - Handle negations ("not support" ‚Üí flipped polarity)
   - Weight by dependency distance and hedges/modals
2. Zero-shot fallback (MNLI):
   - `facebook/bart-large-mnli` or `distilbart-mnli`
   - Hypotheses: "The author expresses {support|oppose|neutral} toward {ENTITY}."
3. Combined scoring:
   - Agreement ‚Üí high confidence; conflicts ‚Üí downgrade to `neutral`/`unclear`

### Event graph
For each message, create edges:
```
Author ‚Üí [stance edge] ‚Üí Entity
```
Each edge includes: `label`, `score`, and `evidence_spans`.

### Config example
```yaml
models:
  stance: facebook/bart-large-mnli  # or distilbart-mnli
processing:
  stance:
    threshold: 0.6
    dep_rules_weight: 0.6
    mnli_weight: 0.4
    negative_words: [not, never, no]
    support_verbs: [support, back, endorse, praise]
    oppose_verbs: [oppose, criticize, condemn, attack]
```

## Topics ‚Äî Hybrid Approach

### Steps
1. Ontology-based classification
   - Curated `config/topics.json` with keywords; stable for longitudinal charts
2. Unsupervised discovery
   - Sentence embeddings + clustering (default: KMeans; optional: BERTopic/HDBSCAN)
   - Extract key phrases per cluster
3. Mapping clusters to ontology
   - Zero-shot classification of clusters against ontology; unmatched ‚Üí `NEW_TOPIC_X`
4. Output fields
   - Store both `ontology_topics` and `discovery_topics` per message

### Example ontology
```json
[
  {"label": "immigration", "keywords": ["immigration", "border"]},
  {"label": "economy", "keywords": ["jobs", "inflation"]}
]
```

## Evaluation and Calibration

### Gold Set Creation
Create 200-300 manually annotated messages with:
- Span-level speaker attribution
- Target entity identification  
- Stance labels (support/oppose/neutral/unclear)

### Metrics
- **Attribution accuracy**: Correct speaker credited
- **Entity accuracy**: Correct target identified
- **Stance precision/recall**: Favor precision for support/oppose labels

### Evaluation Commands
```bash
# Create gold standard annotations
python scripts/create_gold_set.py --input data/sample.json --output data/gold.csv

# Run evaluation
python scripts/eval.py \
  --gold data/gold.csv \
  --pred out/run-*/posts_enriched.parquet \
  --metrics stance entity attribution

# Generate calibration report
python scripts/calibrate.py \
  --eval-results eval_output.json \
  --config config/config.yaml
```

### Calibration Workflow
1. Adjust `stance_threshold` based on precision/recall trade-off
2. Update `aliases.json` with missed entity mappings
3. Refine `topics.json` ontology from discovery clusters
4. Retrain or switch models if systematic bias detected

## Milestones

| Milestone | Deliverable | Acceptance Criteria | Artifacts |
| --------- | ----------- | ------------------- | --------- |
| M0 | Loader + NER + aliasing | JSON/CSV loading, entity extraction >90% accuracy | `loaders.py`, `ner.py`, `aliases.json` |
| M1 | Sentiment + toxicity + style | Sentiment classification, toxicity scoring | `sentiment.py`, `style_features.py` |
| M2 | Quote detection + speaker spans | Quote/forward detection, span attribution | `quote_handler.py`, span tagging tests |
| M3 | Dependency-based stance | Rule-based stance >70% precision | `stance_rules.py`, dependency patterns |
| M4 | Zero-shot stance integration | Hybrid stance >75% precision | `stance_hybrid.py`, MNLI integration |
| M5 | Topic hybrid system (ontology + discovery) | Ontology + discovery pipeline | `topic_classifier.py`, `topics.json` |
| M6 | Aggregations + sidecars | Daily/entity aggregations, CSV outputs | `aggregations.py`, sidecar CSVs |
| M7 | Validation + calibration | Gold set evaluation, metrics >threshold | Evaluation scripts, calibration reports |
| M8 | Optional dashboards | Interactive visualizations | Dashboard code, deployment config |

## Risks & Mitigations

| Risk                         | Mitigation                                             |
| ---------------------------- | ------------------------------------------------------ |
| Over-attribution from quotes | Strict default: exclude quotes unless explicit framing |
| Model bias                   | Document versions and thresholds; retrain periodically |
| Entity explosion             | Cap entities per message; prioritize canonical ones    |
| Runtime blowups              | Text clipping, batching, smaller MNLI models           |
| Dependency bloat             | Optional features, extras, CI size gate                |
| GPU variance                 | CPU-first defaults, runtime capability detection       |
| Data changes in Telegram exports | Robust normalizers, schema validation               |
| Misattribution due to quotes/forwards | Strict evidence retention, default unclear           |
| Model version drift         | Pin model versions, validation against gold standard   |
| Configuration drift         | Config snapshots, automated conformance checks        |

## Performance, Caching, and Batch Sizing

### Batch Size Guidelines
- **CPU**: 8-32 messages (start with 16)
- **Mid-range GPU**: 32-64 messages  
- **High-end GPU**: 64-128 messages

### Memory Management
```yaml
processing:
  batch_size: 16              # Reduce if OOM errors
  max_text_length: 8192       # Clip very long messages
  max_entities_per_msg: 3     # Prevent entity explosion
```

### Caching Strategy
```bash
# Persistent model cache
export HF_HOME=/path/to/persistent/cache
export TRANSFORMERS_CACHE=$HF_HOME/transformers

# Pre-download models to avoid runtime delays
python scripts/download_models.py --config config/config.yaml
```

### Performance Tuning
- Use `distilbart-mnli` instead of `bart-large-mnli` for 3x speedup
- Enable mixed precision on CUDA: `torch.cuda.amp.autocast()`
- Consider multiprocessing for I/O-bound stages (JSON parsing)

## Security, Privacy, and Governance

### Data Handling
- **Public data only**: No private channels or DMs
- **PII limitation**: Only Telegram handles present in source
- **Log redaction**: Sensitive fields configurable via `--redact-logs`

### Reproducibility
```bash
# Pin model versions in config
models:
  ner: dslim/bert-base-NER@sha256:abc123...

# Store config snapshot with every run
cp config/config.yaml out/run-*/config_snapshot.yaml
```

### Supply Chain Security
```bash
# Regular dependency audits
pip-audit --req requirements.txt --format json

# Optional static analysis
bandit -r src/ -f json -o security_report.json
safety check --json
```

### Ethics Checklist
- [ ] Conservative stance labeling (unclear > wrong label)
- [ ] Evidence spans retained for audit trail
- [ ] No sensitive personal information stored
- [ ] Model bias documented and monitored
- [ ] Regular evaluation against gold standard

## CI, Linting, Tests, and Package Pinning

### Pre-commit Hooks (`.pre-commit-config.yaml`)
```yaml
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.6
    hooks:
      - id: ruff
      - id: ruff-format
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: check-yaml
      - id: end-of-file-fixer
```

### Testing Strategy
```bash
# Unit tests for core functions
pytest tests/test_loaders.py -v
pytest tests/test_ner_aliasing.py -v
pytest tests/test_quote_detection.py -v
pytest tests/test_stance_rules.py -v

# Integration test on sample data
pytest tests/test_integration.py --input data/sample.json

# Coverage report
pytest --cov=src tests/ --cov-report=html
```

### Package Pinning Workflow
```bash
# Update dependencies
pip-compile --upgrade requirements.in

# Security audit
pip-audit --req requirements.txt

# Test with new versions
pytest tests/

# Commit if passing
git add requirements.txt && git commit -m "deps: update requirements"
```

## Troubleshooting and FAQs

### GPU Issues
**Q: `torch.cuda.is_available()` returns False**
```bash
# Check CUDA installation
nvidia-smi
nvcc --version

# Reinstall PyTorch with CUDA support
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

**Q: Apple MPS not working**
```bash
# Enable fallback for unsupported ops
export PYTORCH_ENABLE_MPS_FALLBACK=1

# Check MPS availability
python -c "import torch; print(torch.backends.mps.is_available())"
```

### Model Issues
**Q: spaCy model missing**
```bash
python -m spacy download en_core_web_sm
# Or use pip: pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1.tar.gz
```

**Q: Hugging Face model download fails**
```bash
# Manual download
python -c "from transformers import AutoModel; AutoModel.from_pretrained('dslim/bert-base-NER')"

# Use offline cache
export TRANSFORMERS_OFFLINE=1
export HF_DATASETS_OFFLINE=1
```

### Memory Issues
**Q: Out of memory during processing**
- Reduce `batch_size` to 8 or 4
- Set `max_text_length: 4096`
- Use smaller models: `distilbert-base-cased` instead of `bert-base-cased`
- Enable gradient checkpointing if using training mode

**Q: BERTopic consumes too much memory**
```yaml
# Disable topic discovery
processing:
  enable_topic_discovery: false
  
# Or use lighter clustering
topic_discovery:
  clustering_method: kmeans  # instead of hdbscan
  n_topics: 20               # limit topic count
```

### Data Issues
**Q: Language detection unstable on short texts**
- Set `skip_langdetect: true` for monolingual channels
- Increase minimum text length filter

**Q: Quote detection missing quoted content**
- Check for non-standard quote characters: `"` vs `"`
- Enable debug logging: `--log-level DEBUG`
- Review quote detection regex patterns

**Q: Stance classification seems biased**
- Lower `stance_threshold` from 0.6 to 0.4
- Check for missing entries in `aliases.json`
- Review evidence spans in output for false positives

### Performance Issues  
**Q: Processing very slow**
- Enable GPU: `processing.prefer_gpu: true`
- Use smaller models: `distilbart-mnli` vs `bart-large-mnli`
- Increase `batch_size` if memory allows
- Skip language detection: `skip_langdetect: true`

**Q: High variance in processing time**
- Pre-download models to avoid runtime delays
- Use SSD storage for model cache
- Consider multiprocessing for I/O stages

## Development Commands

```bash
# Environment setup
make setup                    # pyenv + venv + requirements
make download-models         # Pre-download ML models

# Running analysis
make run                     # Default sample run
make run INPUT=data/my.json  # Custom input file

# Quality assurance
make lint                    # ruff + black formatting
make test                    # pytest with coverage
make security-audit         # pip-audit + bandit

# Evaluation
make eval                    # Run evaluation on gold set
make calibrate              # Generate calibration report

# Cleanup
make clean                  # Remove outputs and cache
make clean-models          # Remove downloaded models
```

---

**For detailed specifications, see [`docs/spec.md`](docs/spec.md)**


