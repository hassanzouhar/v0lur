#!/usr/bin/env python3
"""
Data loading utilities for Telegram analysis results.

This module provides functions to discover and load various analysis artifacts
generated by the raigem0n pipeline, including CSV summaries, JSON analyses,
and metadata extraction.
"""

import csv
import json
import os
from datetime import datetime
from functools import lru_cache
from pathlib import Path
from typing import Dict, List, Optional, Union


class AnalysisRun:
    """Represents a single analysis run with its metadata and available files."""
    
    def __init__(self, name: str, path: Path, timestamp: datetime):
        self.name = name
        self.path = path
        self.timestamp = timestamp
        self.available_files = self._check_available_files()
    
    def _check_available_files(self) -> Dict[str, bool]:
        """Check which expected files are present in this run."""
        expected_files = [
            "channel_daily_summary.csv",
            "channel_topic_analysis.json",
            "channel_entity_counts.csv",
            "channel_top_toxic_messages.csv",
            "channel_style_features.json",
        ]
        
        available = {}
        for filename in expected_files:
            file_path = self.path / filename
            available[filename] = file_path.exists() and file_path.stat().st_size > 0
        
        return available
    
    def get_file_path(self, filename: str) -> Optional[Path]:
        """Get the full path to a file if it exists."""
        file_path = self.path / filename
        if file_path.exists():
            return file_path
        return None


def discover_runs(out_dir: str = "out") -> List[AnalysisRun]:
    """
    Discover all available analysis runs in the output directory.
    
    Args:
        out_dir: Directory to scan for runs (default: "out")
    
    Returns:
        List of AnalysisRun objects, sorted by timestamp descending
    """
    out_path = Path(out_dir)
    
    if not out_path.exists():
        return []
    
    runs = []
    
    for item in out_path.iterdir():
        if item.is_dir() and not item.name.startswith('.'):
            # Try to parse timestamp from directory name
            timestamp = _parse_timestamp_from_name(item.name)
            
            # Fallback to directory modification time
            if timestamp is None:
                timestamp = datetime.fromtimestamp(item.stat().st_mtime)
            
            run = AnalysisRun(name=item.name, path=item, timestamp=timestamp)
            runs.append(run)
    
    # Sort by timestamp, newest first
    runs.sort(key=lambda r: r.timestamp, reverse=True)
    
    return runs


def _parse_timestamp_from_name(name: str) -> Optional[datetime]:
    """
    Try to parse timestamp from run directory name.
    
    Expected formats:
    - run-20241231-1659
    - test-run-20250917-1623
    - run_2024-12-31_23-59
    """
    import re
    
    # Pattern for YYYYMMDD-HHMM format
    pattern1 = r'(\d{8})-(\d{4})'
    match1 = re.search(pattern1, name)
    if match1:
        date_part, time_part = match1.groups()
        try:
            return datetime.strptime(f"{date_part}-{time_part}", "%Y%m%d-%H%M")
        except ValueError:
            pass
    
    # Pattern for YYYY-MM-DD_HH-MM format
    pattern2 = r'(\d{4}-\d{2}-\d{2})_(\d{2}-\d{2})'
    match2 = re.search(pattern2, name)
    if match2:
        date_part, time_part = match2.groups()
        try:
            return datetime.strptime(f"{date_part} {time_part.replace('-', ':')}", "%Y-%m-%d %H:%M")
        except ValueError:
            pass
    
    return None


@lru_cache(maxsize=32)
def _load_file_cached(file_path: str, mtime: float) -> Union[Dict, List, None]:
    """
    Cache file loading based on path and modification time.
    
    Args:
        file_path: Path to the file
        mtime: File modification time for cache invalidation
    
    Returns:
        Parsed file content or None if loading failed
    """
    path = Path(file_path)
    
    try:
        if path.suffix == '.json':
            with open(path, 'r', encoding='utf-8') as f:
                return json.load(f)
        elif path.suffix == '.csv':
            rows = []
            with open(path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    rows.append(row)
            return rows
        else:
            return None
    except (FileNotFoundError, json.JSONDecodeError, csv.Error, UnicodeDecodeError) as e:
        print(f"Error loading {file_path}: {e}")
        return None


def load_daily_summary(file_path: Path) -> Optional[Dict]:
    """
    Load and process daily summary CSV.
    
    Expected columns: date, message_count, avg_text_length, avg_entities,
                     avg_sentiment, avg_toxicity, max_toxicity
    
    Returns:
        Dict with 'rows' (list of daily data) and 'totals' (aggregated stats)
    """
    if not file_path.exists():
        return None
    
    mtime = file_path.stat().st_mtime
    raw_data = _load_file_cached(str(file_path), mtime)
    
    if not raw_data:
        return None
    
    # Convert numeric fields
    processed_rows = []
    total_messages = 0
    total_sentiment = 0
    total_toxicity = 0
    date_range = []
    
    for row in raw_data:
        try:
            # Convert numeric fields with error handling
            messages = int(float(row.get('message_count', 0)))
            sentiment = float(row.get('avg_sentiment', 0))
            toxicity = float(row.get('avg_toxicity', 0))
            max_toxicity = float(row.get('max_toxicity', 0))
            
            processed_row = {
                'date': row.get('date', ''),
                'message_count': messages,
                'avg_sentiment': sentiment,
                'avg_toxicity': toxicity,
                'max_toxicity': max_toxicity,
                'avg_text_length': float(row.get('avg_text_length', 0)),
                'avg_entities': float(row.get('avg_entities', 0)),
            }
            processed_rows.append(processed_row)
            
            # Accumulate totals
            total_messages += messages
            total_sentiment += sentiment * messages  # Weight by message count
            total_toxicity += toxicity * messages
            date_range.append(row.get('date', ''))
            
        except (ValueError, TypeError):
            continue  # Skip malformed rows
    
    # Calculate weighted averages
    if total_messages > 0:
        avg_sentiment = total_sentiment / total_messages
        avg_toxicity = total_toxicity / total_messages
    else:
        avg_sentiment = 0
        avg_toxicity = 0
    
    # Sort dates for range
    date_range.sort()
    date_range_str = ""
    if date_range:
        if len(date_range) == 1:
            date_range_str = date_range[0]
        else:
            date_range_str = f"{date_range[0]} to {date_range[-1]}"
    
    return {
        'rows': processed_rows,
        'totals': {
            'total_messages': total_messages,
            'avg_sentiment': avg_sentiment,
            'avg_toxicity': avg_toxicity,
            'date_range': date_range_str,
            'num_days': len(processed_rows)
        }
    }


def load_topic_analysis(file_path: Path) -> Optional[Dict]:
    """
    Load and process topic analysis JSON.
    
    Returns:
        Dict with topic distribution and metadata
    """
    if not file_path.exists():
        return None
    
    mtime = file_path.stat().st_mtime
    raw_data = _load_file_cached(str(file_path), mtime)
    
    if not raw_data or not isinstance(raw_data, dict):
        return None
    
    # Extract topic distribution
    summary_stats = raw_data.get('summary_stats', {})
    topic_distribution = summary_stats.get('topic_distribution', {})
    
    # Convert to list with percentages
    topics = []
    total_messages = summary_stats.get('total_messages', 0)
    
    for topic, count_str in topic_distribution.items():
        try:
            count = int(count_str)
            percentage = (count / total_messages * 100) if total_messages > 0 else 0
            
            topics.append({
                'topic': topic.replace('_', ' ').title(),
                'count': count,
                'percentage': percentage,
                'confidence': summary_stats.get('avg_confidence', 0)  # Global avg for now
            })
        except (ValueError, TypeError):
            continue
    
    # Sort by count descending
    topics.sort(key=lambda x: x['count'], reverse=True)
    
    return {
        'topics': topics,
        'metadata': {
            'total_messages': total_messages,
            'unique_topics': summary_stats.get('unique_topics_found', 0),
            'avg_confidence': summary_stats.get('avg_confidence', 0),
            'confident_percentage': summary_stats.get('confident_percentage', 0)
        }
    }


def load_entity_counts(file_path: Path) -> Optional[Dict]:
    """
    Load and process entity counts CSV.
    
    Expected columns: entity, count
    
    Returns:
        Dict with sorted entities and metadata
    """
    if not file_path.exists():
        return None
    
    mtime = file_path.stat().st_mtime
    raw_data = _load_file_cached(str(file_path), mtime)
    
    if not raw_data:
        return None
    
    # Process entities
    entities = []
    total_mentions = 0
    
    for row in raw_data:
        try:
            entity = row.get('entity', '').strip()
            count = int(row.get('count', 0))
            
            if entity and count > 0:
                entities.append({
                    'entity': entity,
                    'count': count
                })
                total_mentions += count
                
        except (ValueError, TypeError):
            continue
    
    # Sort by count and add percentages
    entities.sort(key=lambda x: x['count'], reverse=True)
    
    for entity in entities:
        entity['percentage'] = (entity['count'] / total_mentions * 100) if total_mentions > 0 else 0
    
    return {
        'entities': entities,
        'metadata': {
            'total_entities': len(entities),
            'total_mentions': total_mentions
        }
    }


def load_toxic_messages(file_path: Path) -> Optional[Dict]:
    """
    Load and process top toxic messages CSV.
    
    Returns:
        Dict with sorted toxic messages
    """
    if not file_path.exists():
        return None
    
    mtime = file_path.stat().st_mtime
    raw_data = _load_file_cached(str(file_path), mtime)
    
    if not raw_data:
        return None
    
    # Process messages
    messages = []
    
    for row in raw_data:
        try:
            # Handle different possible column names
            msg_id = row.get('msg_id') or row.get('message_id') or row.get('id', '')
            toxicity = float(row.get('toxicity_score', 0))
            text = row.get('text', '').strip()
            date = row.get('date', '')
            author = row.get('author', 'Unknown')
            
            if text and toxicity > 0:
                # Create text preview (first 100 chars)
                text_preview = text[:100] + "..." if len(text) > 100 else text
                
                messages.append({
                    'msg_id': str(msg_id),
                    'toxicity_score': toxicity,
                    'date': date,
                    'author': author,
                    'text_preview': text_preview,
                    'full_text': text
                })
                
        except (ValueError, TypeError):
            continue
    
    # Sort by toxicity score descending
    messages.sort(key=lambda x: x['toxicity_score'], reverse=True)
    
    return {
        'messages': messages[:50],  # Top 50 most toxic
        'metadata': {
            'total_messages': len(messages),
            'max_toxicity': max([m['toxicity_score'] for m in messages], default=0),
            'avg_toxicity': sum([m['toxicity_score'] for m in messages]) / len(messages) if messages else 0
        }
    }


def load_style_features(file_path: Path) -> Optional[Dict]:
    """
    Load and process style features JSON.
    
    Returns:
        Dict with flattened feature key-value pairs
    """
    if not file_path.exists():
        return None
    
    mtime = file_path.stat().st_mtime
    raw_data = _load_file_cached(str(file_path), mtime)
    
    if not raw_data or not isinstance(raw_data, dict):
        return None
    
    def flatten_dict(d: dict, parent_key: str = '', sep: str = '.') -> dict:
        """Recursively flatten nested dictionary."""
        items = []
        for k, v in d.items():
            new_key = f"{parent_key}{sep}{k}" if parent_key else k
            if isinstance(v, dict):
                items.extend(flatten_dict(v, new_key, sep=sep).items())
            elif isinstance(v, (int, float, str)):
                items.append((new_key, v))
        return dict(items)
    
    # Flatten the features
    flattened = flatten_dict(raw_data)
    
    # Convert to list of feature objects
    features = []
    for key, value in flattened.items():
        # Skip metadata fields
        if key in ['extraction_date', 'processor_version', 'channel_name']:
            continue
            
        # Format the key for display
        display_key = key.replace('_', ' ').title()
        
        # Format the value
        if isinstance(value, float):
            formatted_value = f"{value:.3f}"
        elif isinstance(value, int):
            formatted_value = str(value)
        else:
            formatted_value = str(value)
        
        features.append({
            'feature': display_key,
            'value': formatted_value,
            'raw_value': value
        })
    
    # Sort alphabetically by feature name
    features.sort(key=lambda x: x['feature'])
    
    return {
        'features': features,
        'metadata': {
            'total_features': len(features),
            'extraction_date': raw_data.get('extraction_date', ''),
        }
    }